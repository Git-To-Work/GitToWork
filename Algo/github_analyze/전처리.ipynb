{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양쪽 모두 존재하는 회사명 목록:\n",
      "펨트론\n",
      "한국네트웍스\n",
      "소프트넷\n",
      "와탭랩스\n",
      "피앤이시스템즈\n",
      "호전실업\n",
      "티사이언티픽\n",
      "소만사\n",
      "네비웍스\n",
      "펜타시큐리티\n",
      "미스터블루\n",
      "위즈코리아\n",
      "링네트\n",
      "덴티움\n",
      "이파피루스\n",
      "이수시스템\n",
      "엑심베이\n",
      "메타넷티플랫폼\n",
      "네오티스\n",
      "디지털포토\n",
      "콘텐츠웨이브\n",
      "크래프톤\n",
      "넥서스커뮤니티\n",
      "유진로봇\n",
      "쎄크\n",
      "씨브이네트\n",
      "코아비스\n",
      "디에스이트레이드\n",
      "인피닉\n",
      "위드네트웍스\n",
      "앤씨앤\n",
      "씨이랩\n",
      "이노웍스\n",
      "레이언스\n",
      "이너버스\n",
      "네오펙트\n",
      "이노아이\n",
      "아이파킹\n",
      "이엘피\n",
      "엔미디어플랫폼\n",
      "파이오링크\n",
      "메이아이\n",
      "커넥트웨이브\n",
      "시어스랩\n",
      "스콥정보통신\n",
      "이스트소프트\n",
      "건솔루션\n",
      "와이즈넛\n",
      "대상정보기술\n",
      "한솔테크닉스\n",
      "티엔에이치\n",
      "슈프리마\n",
      "코닉글로리\n",
      "엘아이지시스템\n",
      "유니온커뮤니티\n",
      "메타넷글로벌\n",
      "브레인즈컴퍼니\n",
      "메디트\n",
      "유디아이디\n",
      "코그넷나인\n",
      "오픈소스컨설팅\n",
      "캐럿글로벌\n",
      "엔에이치엔링크\n",
      "아이디에스앤트러스트\n",
      "씨앤유글로벌\n",
      "모비루스\n",
      "날개물류\n",
      "신비앤텍\n",
      "에임메드\n",
      "스톤위즈\n",
      "프레스토솔루션\n",
      "상화\n",
      "비욘드테크\n",
      "메가스터디교육\n",
      "미디어로그\n",
      "이글루코퍼레이션\n",
      "케이웨더\n",
      "페니로이스\n",
      "지2터치\n",
      "씨알에스큐브\n",
      "누리인포스\n",
      "총 개수: 81\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def load_company_names(folder_path, key='companyName'):\n",
    "    company_names = set()\n",
    "    # 폴더 내 모든 .json 파일 경로 찾기\n",
    "    json_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "    for file in json_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            # 만약 파일 내에 companyName 필드가 존재하면 추가\n",
    "            if key in data and data[key]:\n",
    "                company_names.add(data[key])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    return company_names\n",
    "\n",
    "# 폴더 경로 설정\n",
    "folder1 = './crawling'\n",
    "folder2 = './jobs'\n",
    "\n",
    "# 각 폴더의 회사명 로드\n",
    "companies1 = load_company_names(folder1)\n",
    "companies2 = load_company_names(folder2)\n",
    "\n",
    "# 양쪽 모두 존재하는 회사명 구하기\n",
    "common_companies = companies1.intersection(companies2)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"양쪽 모두 존재하는 회사명 목록:\")\n",
    "for company in common_companies:\n",
    "    print(company)\n",
    "print(f\"총 개수: {len(common_companies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique companyName count: 3737\n",
      "JSON file created: unique_company_names_jobkorea.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def load_unique_company_names(folder_path, key='companyName'):\n",
    "    company_names = set()\n",
    "    # 폴더 내 모든 .json 파일 경로 찾기\n",
    "    json_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "    for file in json_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            # JSON 파일에 companyName 필드가 존재하면 추가\n",
    "            if key in data and data[key]:\n",
    "                company_names.add(data[key])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    return company_names\n",
    "\n",
    "# .jobs 폴더 경로 설정\n",
    "folder = './jobs'\n",
    "unique_companies = load_unique_company_names(folder)\n",
    "unique_count = len(unique_companies)\n",
    "\n",
    "# 결과 출력: 고유한 companyName의 개수\n",
    "print(f\"Unique companyName count: {unique_count}\")\n",
    "\n",
    "# 결과를 JSON 형식으로 저장할 데이터 구성\n",
    "output = {\n",
    "    \"unique_companyName_count\": unique_count,\n",
    "    \"companyNames\": list(unique_companies)\n",
    "}\n",
    "\n",
    "# 결과 JSON 파일로 저장\n",
    "output_file = 'unique_company_names_jumpit.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON file created: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도가 높은(동일 가능성이 있는) 회사명 쌍:\n",
      "{'jumpit': '한국네트웍스', 'jobkorea': '한국네트웍스', 'similarity': 100.0}\n",
      "{'jumpit': '소프트넷', 'jobkorea': '소프트넷', 'similarity': 100.0}\n",
      "{'jumpit': '호전실업', 'jobkorea': '호전실업', 'similarity': 100.0}\n",
      "{'jumpit': '펜타시큐리티', 'jobkorea': '펜타시큐리티', 'similarity': 100.0}\n",
      "{'jumpit': '네오티스', 'jobkorea': '네오티스', 'similarity': 100.0}\n",
      "{'jumpit': '와이즈스톤티', 'jobkorea': '와이즈스톤', 'similarity': 90.9090909090909}\n",
      "{'jumpit': '코아비스', 'jobkorea': '코아비스', 'similarity': 100.0}\n",
      "{'jumpit': '위드네트웍스', 'jobkorea': '위드네트웍스', 'similarity': 100.0}\n",
      "{'jumpit': '이노아이', 'jobkorea': '이노아이', 'similarity': 100.0}\n",
      "{'jumpit': '아이파킹', 'jobkorea': '아이파킹', 'similarity': 100.0}\n",
      "{'jumpit': '이엘피', 'jobkorea': '이엘피', 'similarity': 100.0}\n",
      "{'jumpit': '엔미디어플랫폼', 'jobkorea': '엔미디어플랫폼', 'similarity': 100.0}\n",
      "{'jumpit': '파이오링크', 'jobkorea': '파이오링크', 'similarity': 100.0}\n",
      "{'jumpit': '시어스랩', 'jobkorea': '시어스랩', 'similarity': 100.0}\n",
      "{'jumpit': '건솔루션', 'jobkorea': '건솔루션', 'similarity': 100.0}\n",
      "{'jumpit': '유니온커뮤니티', 'jobkorea': '유니온커뮤니티', 'similarity': 100.0}\n",
      "{'jumpit': '메타넷글로벌', 'jobkorea': '메타넷글로벌', 'similarity': 100.0}\n",
      "{'jumpit': '메디트', 'jobkorea': '메디트', 'similarity': 100.0}\n",
      "{'jumpit': '엔에이치엔링크', 'jobkorea': '엔에이치엔링크', 'similarity': 100.0}\n",
      "{'jumpit': '모비루스', 'jobkorea': '모비루스', 'similarity': 100.0}\n",
      "{'jumpit': '미스터블루', 'jobkorea': '미스터블루', 'similarity': 100.0}\n",
      "{'jumpit': '위즈코리아', 'jobkorea': '위즈코리아', 'similarity': 100.0}\n",
      "{'jumpit': '링네트', 'jobkorea': '링네트', 'similarity': 100.0}\n",
      "{'jumpit': '콘텐츠웨이브', 'jobkorea': '콘텐츠웨이브', 'similarity': 100.0}\n",
      "{'jumpit': '크래프톤', 'jobkorea': '크래프톤', 'similarity': 100.0}\n",
      "{'jumpit': '씨브이네트', 'jobkorea': '씨브이네트', 'similarity': 100.0}\n",
      "{'jumpit': '앤씨앤', 'jobkorea': '앤씨앤', 'similarity': 100.0}\n",
      "{'jumpit': '네오펙트', 'jobkorea': '네오펙트', 'similarity': 100.0}\n",
      "{'jumpit': '한솔테크닉스', 'jobkorea': '한솔테크닉스', 'similarity': 100.0}\n",
      "{'jumpit': '슈프리마', 'jobkorea': '슈프리마', 'similarity': 100.0}\n",
      "{'jumpit': '코닉글로리', 'jobkorea': '코닉글로리', 'similarity': 100.0}\n",
      "{'jumpit': '유디아이디', 'jobkorea': '유디아이디', 'similarity': 100.0}\n",
      "{'jumpit': '오픈소스컨설팅', 'jobkorea': '오픈소스컨설팅', 'similarity': 100.0}\n",
      "{'jumpit': '아이디에스앤트러스트', 'jobkorea': '아이디에스앤트러스트', 'similarity': 100.0}\n",
      "{'jumpit': '상화', 'jobkorea': '상화', 'similarity': 100.0}\n",
      "{'jumpit': '미디어로그', 'jobkorea': '미디어로그', 'similarity': 100.0}\n",
      "{'jumpit': '페니로이스', 'jobkorea': '페니로이스', 'similarity': 100.0}\n",
      "{'jumpit': '씨알에스큐브', 'jobkorea': '씨알에스큐브', 'similarity': 100.0}\n",
      "{'jumpit': '와탭랩스', 'jobkorea': '와탭랩스', 'similarity': 100.0}\n",
      "{'jumpit': '티사이언티픽', 'jobkorea': '티사이언티픽', 'similarity': 100.0}\n",
      "{'jumpit': '이수시스템', 'jobkorea': '이수시스템', 'similarity': 100.0}\n",
      "{'jumpit': '엑심베이', 'jobkorea': '엑심베이', 'similarity': 100.0}\n",
      "{'jumpit': '메타넷티플랫폼', 'jobkorea': '메타넷티플랫폼', 'similarity': 100.0}\n",
      "{'jumpit': '비에이치에스티', 'jobkorea': '비에이치에스', 'similarity': 92.3076923076923}\n",
      "{'jumpit': '디지털포토', 'jobkorea': '디지털포토', 'similarity': 100.0}\n",
      "{'jumpit': '넥서스커뮤니티', 'jobkorea': '넥서스커뮤니티', 'similarity': 100.0}\n",
      "{'jumpit': '인피닉', 'jobkorea': '인피닉', 'similarity': 100.0}\n",
      "{'jumpit': '이노웍스', 'jobkorea': '이노웍스', 'similarity': 100.0}\n",
      "{'jumpit': '메이아이', 'jobkorea': '메이아이', 'similarity': 100.0}\n",
      "{'jumpit': '이스트소프트', 'jobkorea': '이스트소프트', 'similarity': 100.0}\n",
      "{'jumpit': '와이즈넛', 'jobkorea': '와이즈넛', 'similarity': 100.0}\n",
      "{'jumpit': '스톤위즈', 'jobkorea': '스톤위즈', 'similarity': 100.0}\n",
      "{'jumpit': '프레스토솔루션', 'jobkorea': '프레스토솔루션', 'similarity': 100.0}\n",
      "{'jumpit': '비욘드테크', 'jobkorea': '비욘드테크', 'similarity': 100.0}\n",
      "{'jumpit': '이글루코퍼레이션', 'jobkorea': '이글루코퍼레이션', 'similarity': 100.0}\n",
      "{'jumpit': '지2터치', 'jobkorea': '지2터치', 'similarity': 100.0}\n",
      "{'jumpit': '펨트론', 'jobkorea': '펨트론', 'similarity': 100.0}\n",
      "{'jumpit': '피앤이시스템즈', 'jobkorea': '피앤이시스템즈', 'similarity': 100.0}\n",
      "{'jumpit': '소만사', 'jobkorea': '소만사', 'similarity': 100.0}\n",
      "{'jumpit': '네비웍스', 'jobkorea': '네비웍스', 'similarity': 100.0}\n",
      "{'jumpit': '덴티움', 'jobkorea': '덴티움', 'similarity': 100.0}\n",
      "{'jumpit': '이파피루스', 'jobkorea': '이파피루스', 'similarity': 100.0}\n",
      "{'jumpit': '유진로봇', 'jobkorea': '유진로봇', 'similarity': 100.0}\n",
      "{'jumpit': '쎄크', 'jobkorea': '쎄크', 'similarity': 100.0}\n",
      "{'jumpit': '디에스이트레이드', 'jobkorea': '디에스이트레이드', 'similarity': 100.0}\n",
      "{'jumpit': '씨이랩', 'jobkorea': '씨이랩', 'similarity': 100.0}\n",
      "{'jumpit': '레이언스', 'jobkorea': '레이언스', 'similarity': 100.0}\n",
      "{'jumpit': '이너버스', 'jobkorea': '이너버스', 'similarity': 100.0}\n",
      "{'jumpit': '커넥트웨이브', 'jobkorea': '커넥트웨이브', 'similarity': 100.0}\n",
      "{'jumpit': '스콥정보통신', 'jobkorea': '스콥정보통신', 'similarity': 100.0}\n",
      "{'jumpit': '대상정보기술', 'jobkorea': '대상정보기술', 'similarity': 100.0}\n",
      "{'jumpit': '티엔에이치', 'jobkorea': '티엔에이치', 'similarity': 100.0}\n",
      "{'jumpit': '엘아이지시스템', 'jobkorea': '엘아이지시스템', 'similarity': 100.0}\n",
      "{'jumpit': '브레인즈컴퍼니', 'jobkorea': '브레인즈컴퍼니', 'similarity': 100.0}\n",
      "{'jumpit': '코그넷나인', 'jobkorea': '코그넷나인', 'similarity': 100.0}\n",
      "{'jumpit': '캐럿글로벌', 'jobkorea': '캐럿글로벌', 'similarity': 100.0}\n",
      "{'jumpit': '씨앤유글로벌', 'jobkorea': '씨앤유글로벌', 'similarity': 100.0}\n",
      "{'jumpit': '신비앤텍', 'jobkorea': '신비앤텍', 'similarity': 100.0}\n",
      "{'jumpit': '날개물류', 'jobkorea': '날개물류', 'similarity': 100.0}\n",
      "{'jumpit': '에임메드', 'jobkorea': '에임메드', 'similarity': 100.0}\n",
      "{'jumpit': '메가스터디교육', 'jobkorea': '메가스터디교육', 'similarity': 100.0}\n",
      "{'jumpit': '케이웨더', 'jobkorea': '케이웨더', 'similarity': 100.0}\n",
      "{'jumpit': '지피에이코리아', 'jobkorea': '지에이코리아', 'similarity': 92.3076923076923}\n",
      "{'jumpit': '누리인포스', 'jobkorea': '누리인포스', 'similarity': 100.0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# JSON 파일에서 회사명 리스트 로드\n",
    "with open(\"unique_company_names_jumpit.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jumpit_data = json.load(f)\n",
    "jumpit_names = jumpit_data[\"companyNames\"]\n",
    "\n",
    "with open(\"unique_company_names_jobkorea.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jobkorea_data = json.load(f)\n",
    "jobkorea_names = jobkorea_data[\"companyNames\"]\n",
    "\n",
    "# 회사명 비교 시, 간단한 정규화를 수행할 수 있음 (소문자화, 공백 제거 등)\n",
    "def normalize(name):\n",
    "    return name.lower().replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "# 유사도가 높은 회사명 쌍을 찾기 위한 임계치 설정 (0~100 사이)\n",
    "threshold = 90\n",
    "suspected_duplicates = []\n",
    "\n",
    "# 두 리스트의 모든 회사명을 비교 (계산량이 많을 수 있으므로 필요에 따라 최적화)\n",
    "for name1 in jumpit_names:\n",
    "    norm1 = normalize(name1)\n",
    "    for name2 in jobkorea_names:\n",
    "        norm2 = normalize(name2)\n",
    "        similarity = fuzz.ratio(norm1, norm2)\n",
    "        if similarity >= threshold:\n",
    "            suspected_duplicates.append({\n",
    "                \"jumpit\": name1,\n",
    "                \"jobkorea\": name2,\n",
    "                \"similarity\": similarity\n",
    "            })\n",
    "\n",
    "# 결과 출력\n",
    "print(\"유사도가 높은(동일 가능성이 있는) 회사명 쌍:\")\n",
    "for pair in suspected_duplicates:\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized JSON file created: unique_company_names_jumpit_normalized.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 정규화 함수: 괄호와 그 안의 내용을 제거\n",
    "def normalize_company_name(name):\n",
    "    # 괄호와 그 내부 내용 제거 (예: \"아이센스(caresens)\" -> \"아이센스\")\n",
    "    normalized = re.sub(r'\\(.*?\\)', '', name)\n",
    "    # 좌우 공백 제거\n",
    "    return normalized.strip()\n",
    "\n",
    "# 기존 JSON 파일 로드 (unique_company_names_jumpit.json)\n",
    "input_file = \"unique_company_names_jumpit.json\"\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 회사명 리스트 수정: 각각 정규화 진행\n",
    "original_names = data[\"companyNames\"]\n",
    "normalized_names = [normalize_company_name(name) for name in original_names]\n",
    "\n",
    "# 수정된 결과로 새로운 JSON 데이터 생성 (개수도 업데이트)\n",
    "normalized_data = {\n",
    "    \"unique_companyName_count\": len(set(normalized_names)),\n",
    "    \"companyNames\": list(set(normalized_names))\n",
    "}\n",
    "\n",
    "# 결과 저장할 파일명 지정\n",
    "output_file = \"unique_company_names_jumpit_normalized.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(normalized_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Normalized JSON file created: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"WATA Inc.\"을(를) 포함한 JSON 파일:\n",
      "./jobs\\job_43856.json\n",
      "./jobs\\job_43857.json\n",
      "./jobs\\job_43858.json\n",
      "./jobs\\job_44510.json\n",
      "./jobs\\job_44511.json\n",
      "./jobs\\job_44512.json\n",
      "./jobs\\job_44513.json\n",
      "./jobs\\job_44514.json\n",
      "./jobs\\job_44515.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# 검색할 타겟 회사명\n",
    "target_name = \"WATA Inc.\"\n",
    "# 검색할 폴더 경로 (필요에 따라 수정)\n",
    "folder_path = \"./jobs\"  # 현재 폴더 또는 원하는 경로\n",
    "\n",
    "# 폴더 내 모든 JSON 파일 목록 가져오기\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "matching_files = []\n",
    "\n",
    "for file in json_files:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # companyName 키가 있고, 값이 타겟과 일치하면 해당 파일명을 저장\n",
    "        if data.get(\"companyName\") == target_name:\n",
    "            matching_files.append(file)\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 중 오류 발생 ({file}): {e}\")\n",
    "\n",
    "if matching_files:\n",
    "    print(f'\"{target_name}\"을(를) 포함한 JSON 파일:')\n",
    "    for f in matching_files:\n",
    "        print(f)\n",
    "else:\n",
    "    print(f'\"{target_name}\"을(를) 포함한 JSON 파일을 찾지 못했습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique companyName count: 550\n",
      "JSON file created: unique_company_names_jumpit.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def normalize_company_name(name):\n",
    "    # 괄호와 그 내부 내용을 제거한 후 좌우 공백 제거 (예: \"아이센스(caresens)\" -> \"아이센스\")\n",
    "    normalized = re.sub(r'\\(.*?\\)', '', name)\n",
    "    return normalized.strip()\n",
    "\n",
    "def load_unique_company_names(folder_path, key='companyName'):\n",
    "    company_names = set()\n",
    "    # 폴더 내 모든 .json 파일 경로 찾기\n",
    "    json_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "    for file in json_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            # JSON 파일에 companyName 필드가 존재하면 정규화 후 추가\n",
    "            if key in data and data[key]:\n",
    "                normalized_name = normalize_company_name(data[key])\n",
    "                company_names.add(normalized_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    return company_names\n",
    "\n",
    "# .jobs 폴더 경로 설정\n",
    "folder = './jobs'\n",
    "unique_companies = load_unique_company_names(folder)\n",
    "unique_count = len(unique_companies)\n",
    "\n",
    "# 결과 출력: 고유한 companyName의 개수\n",
    "print(f\"Unique companyName count: {unique_count}\")\n",
    "\n",
    "# 결과를 JSON 형식으로 저장할 데이터 구성\n",
    "output = {\n",
    "    \"unique_companyName_count\": unique_count,\n",
    "    \"companyNames\": list(unique_companies)\n",
    "}\n",
    "\n",
    "# 결과 JSON 파일로 저장\n",
    "output_file = 'unique_company_names_jumpit.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON file created: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created: collected_target_companies.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# jumpit JSON 파일 로드\n",
    "with open(\"unique_company_names_jumpit.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jumpit_data = json.load(f)\n",
    "jumpit_companies = set(jumpit_data.get(\"companyNames\", []))\n",
    "\n",
    "# jobkorea JSON 파일 로드\n",
    "with open(\"unique_company_names_jobkorea.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jobkorea_data = json.load(f)\n",
    "jobkorea_companies = set(jobkorea_data.get(\"companyNames\", []))\n",
    "\n",
    "# jumpit에만 존재하는 회사명 추출\n",
    "target_companies = jumpit_companies - jobkorea_companies\n",
    "\n",
    "# 결과 JSON 데이터 구성\n",
    "output = {\n",
    "    \"unique_companyName_count\": len(target_companies),\n",
    "    \"companyNames\": list(target_companies)\n",
    "}\n",
    "\n",
    "# 결과를 JSON 파일로 저장\n",
    "output_file = \"collected_target_companies.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON file created: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing companies saved in 'missing_companies.json' (총 211개)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# target JSON 파일에서 기업명 리스트 로드\n",
    "with open(\"collected_target_companies.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    target_data = json.load(f)\n",
    "\n",
    "# 대상 기업명은 \"companyNames\" 리스트에 있다고 가정합니다.\n",
    "# (필요에 따라 키 이름을 조정하세요)\n",
    "target_companies = set(target_data.get(\"companyNames\", []))\n",
    "\n",
    "# 크롤링된 파일이 있는 폴더 (예시: \"./collected_crawling\")\n",
    "crawled_folder = \"./collected_crawling\"\n",
    "\n",
    "# 폴더 내 JSON 파일 이름을 리스트로 가져오기 (확장자 제외)\n",
    "crawled_files = [f for f in os.listdir(crawled_folder) if f.endswith(\".json\")]\n",
    "\n",
    "def normalize_company_name(name):\n",
    "    \"\"\"\n",
    "    회사명에 포함될 수 있는 특수문자를 제거하는 함수.\n",
    "    (크롤링 시 safe_company_name과 동일한 방식으로 처리했다고 가정)\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name).strip()\n",
    "\n",
    "# 크롤링된 회사명 (파일 이름에서 확장자 제거)\n",
    "crawled_companies = set(normalize_company_name(os.path.splitext(filename)[0]) for filename in crawled_files)\n",
    "\n",
    "# 비교: target에는 있지만, 크롤링 폴더에는 없는 기업명\n",
    "missing_companies = list(target_companies - crawled_companies)\n",
    "\n",
    "# 정렬 (선택 사항)\n",
    "missing_companies.sort()\n",
    "\n",
    "# 결과를 JSON 파일로 저장\n",
    "with open(\"missing_companies.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(missing_companies, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Missing companies saved in 'missing_companies.json' (총 {len(missing_companies)}개)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니크 category 개수: 152\n",
      "유니크 category 리스트: ['제약', '원무', '인테리어', '목재', '렌탈', '증권', '패션', '언론사', '섬유', '학습지', '제지', '쇼핑몰', '건축', '바이오', '설비', '정보보안', '모바일', '대출', '자재', '케이블', '생활용품', '홍보', '무역', '중학교', '식품가공', '중개', '자동차', '배송', '조선', '광학', '솔루션', '방송', '이벤트', '부동산', '보안', '여가', '통신서비스', '공기업', '연예', '여행', '보건', '백화점', '배급', '신문', '기계', '회계', 'IT컨설팅', '기계설비', '시공', '법무', '반도체', '출판', '협회', '은행', 'CRM', '호텔', '특수학교', '운송', '제어', '커뮤니티', '의류', '화학', '광고', '하드웨어', '공연', '컨텐츠', '항공', 'ERP', '엔터테인먼트', '카센터', '건설', '임대', '학원', '애니메이션', '웹에이전시', '서치펌', '소셜커머스', '방문교육', '게임', '음식료', '의료(병원분류별)', '카드', '금속', '교육원', '토목', '스포츠', '캐피탈', '장비', '전시', '재료', '잡지', '전자', '연구', '컴퓨터', '전기', '공공기관', '네트워크', '생활화학', '디자인', '소비재', '리스', '철강', '조경', '아웃소싱', '상담', '문화', '환경', '콜센터', '도소매', '오픈마켓', '경비', '어학원', '보험', '금융', '영화', '시설관리', 'A/S', '유통', '헤드헌팅', '의료(진료과별)', '정비', '조사', '사진', '에너지', '화장품', '포털', '상조', 'APP', '외식', '세무', '물류', '요양', '단체', '웨딩', '예술', '가구', '기타', '디스플레이', '프로덕션', '레저', '초등학교', '음반', '상사', 'CAD', '컨설팅', '프랜차이즈', '대학교', '의료(간호)', 'SI', '고등학교', '인쇄', '사회복지']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder_path = './crawling'\n",
    "unique_categories = set()\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if 'category' in data:\n",
    "                # category가 리스트 형태로 있다고 가정\n",
    "                unique_categories.update(data['category'])\n",
    "\n",
    "# 유니크한 category의 개수 출력\n",
    "print(\"유니크 category 개수:\", len(unique_categories))\n",
    "# 유니크한 category 전체 리스트 출력\n",
    "print(\"유니크 category 리스트:\", list(unique_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 JSON 파일에 카테고리 수정 및 대분류 추가 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 수정할 카테고리 값 매핑\n",
    "corrections = {\n",
    "    '학교(초': '학교(초)',\n",
    "    '의료(간호': '의료(간호)',\n",
    "    '상담)': '상담',\n",
    "    '특수)': '특수'\n",
    "}\n",
    "\n",
    "# 대분류 매핑 예시 (필요에 따라 확장하세요)\n",
    "mapping = {\n",
    "    # IT·디지털 및 소프트웨어\n",
    "    'ERP': 'IT·디지털 및 소프트웨어',\n",
    "    'APP': 'IT·디지털 및 소프트웨어',\n",
    "    '모바일': 'IT·디지털 및 소프트웨어',\n",
    "    '솔루션': 'IT·디지털 및 소프트웨어',\n",
    "    'SI': 'IT·디지털 및 소프트웨어',\n",
    "    '보안': 'IT·디지털 및 소프트웨어',\n",
    "    'IT컨설팅': 'IT·디지털 및 소프트웨어',\n",
    "    'CRM': 'IT·디지털 및 소프트웨어',\n",
    "    '컴퓨터': 'IT·디지털 및 소프트웨어',\n",
    "    '정보보안': 'IT·디지털 및 소프트웨어',\n",
    "    '웹에이전시': 'IT·디지털 및 소프트웨어',\n",
    "    '네트워크': 'IT·디지털 및 소프트웨어',\n",
    "    '통신서비스': 'IT·디지털 및 소프트웨어',\n",
    "\n",
    "    # 제조·기계·산업\n",
    "    '기계': '제조·기계·산업',\n",
    "    '자재': '제조·기계·산업',\n",
    "    '하드웨어': '제조·기계·산업',\n",
    "    '전기': '제조·기계·산업',\n",
    "    '제어': '제조·기계·산업',\n",
    "    '기계설비': '제조·기계·산업',\n",
    "    '금속': '제조·기계·산업',\n",
    "    '전자': '제조·기계·산업',\n",
    "    '디스플레이': '제조·기계·산업',\n",
    "    '장비': '제조·기계·산업',\n",
    "    '인쇄': '제조·기계·산업',\n",
    "    '광학': '제조·기계·산업',\n",
    "    '화학': '제조·기계·산업',\n",
    "    '생활화학': '제조·기계·산업',\n",
    "    '재료': '제조·기계·산업',\n",
    "    '설비': '제조·기계·산업',\n",
    "    '환경': '제조·기계·산업',\n",
    "    '목재': '제조·기계·산업',\n",
    "    '섬유': '제조·기계·산업',\n",
    "    '제지': '제조·기계·산업',\n",
    "    '조선': '제조·기계·산업',\n",
    "\n",
    "    # 건설·부동산·인테리어\n",
    "    '토목': '건설·부동산·인테리어',\n",
    "    '임대': '건설·부동산·인테리어',\n",
    "    '부동산': '건설·부동산·인테리어',\n",
    "    '렌탈': '건설·부동산·인테리어',\n",
    "    '건설': '건설·부동산·인테리어',\n",
    "    '중개': '건설·부동산·인테리어',\n",
    "    '건축': '건설·부동산·인테리어',\n",
    "    '인테리어': '건설·부동산·인테리어',\n",
    "    '조경': '건설·부동산·인테리어',\n",
    "\n",
    "    # 금융·보험\n",
    "    '은행': '금융·보험',\n",
    "    '보험': '금융·보험',\n",
    "    '대출': '금융·보험',\n",
    "    '카드': '금융·보험',\n",
    "    '금융': '금융·보험',\n",
    "    '캐피탈': '금융·보험',\n",
    "    '리스': '금융·보험',\n",
    "\n",
    "    # 유통·소매·물류\n",
    "    '도소매': '유통·소매·물류',\n",
    "    '생활용품': '유통·소매·물류',\n",
    "    '의류': '유통·소매·물류',\n",
    "    '백화점': '유통·소매·물류',\n",
    "    '쇼핑몰': '유통·소매·물류',\n",
    "    '소셜커머스': '유통·소매·물류',\n",
    "    '배송': '유통·소매·물류',\n",
    "    '물류': '유통·소매·물류',\n",
    "    '프랜차이즈': '유통·소매·물류',\n",
    "    '가구': '유통·소매·물류',\n",
    "\n",
    "    # 교육·연구\n",
    "    '교육원': '교육·연구',\n",
    "    '방문교육': '교육·연구',\n",
    "    '학교(초)': '교육·연구',\n",
    "    '어학원': '교육·연구',\n",
    "    '대학': '교육·연구',\n",
    "    '학습지': '교육·연구',\n",
    "    '학원': '교육·연구',\n",
    "    '연구': '교육·연구',\n",
    "    '인터넷교육': '교육·연구',\n",
    "\n",
    "    # 의료·보건·복지\n",
    "    '요양': '의료·보건·복지',\n",
    "    '의료(진료과별)': '의료·보건·복지',\n",
    "    '의료(병원분류별)': '의료·보건·복지',\n",
    "    '의료(간호)': '의료·보건·복지',\n",
    "    '보건': '의료·보건·복지',\n",
    "    '제약': '의료·보건·복지',\n",
    "    '바이오': '의료·보건·복지',\n",
    "    '사회복지': '의료·보건·복지',\n",
    "\n",
    "    # 미디어·엔터테인먼트·문화\n",
    "    '영화': '미디어·엔터테인먼트·문화',\n",
    "    '잡지': '미디어·엔터테인먼트·문화',\n",
    "    '출판': '미디어·엔터테인먼트·문화',\n",
    "    '연예': '미디어·엔터테인먼트·문화',\n",
    "    '방송': '미디어·엔터테인먼트·문화',\n",
    "    '음반': '미디어·엔터테인먼트·문화',\n",
    "    '프로덕션': '미디어·엔터테인먼트·문화',\n",
    "    '예술': '미디어·엔터테인먼트·문화',\n",
    "    '전시': '미디어·엔터테인먼트·문화',\n",
    "    '배급': '미디어·엔터테인먼트·문화',\n",
    "    '애니메이션': '미디어·엔터테인먼트·문화',\n",
    "    '신문': '미디어·엔터테인먼트·문화',\n",
    "    '공연': '미디어·엔터테인먼트·문화',\n",
    "    '문화': '미디어·엔터테인먼트·문화',\n",
    "    '스포츠': '미디어·엔터테인먼트·문화',\n",
    "    '사진': '미디어·엔터테인먼트·문화',\n",
    "    '언론사': '미디어·엔터테인먼트·문화',\n",
    "\n",
    "    # 광고·마케팅 및 전문서비스\n",
    "    '홍보': '광고·마케팅 및 전문서비스',\n",
    "    '광고': '광고·마케팅 및 전문서비스',\n",
    "    '이벤트': '광고·마케팅 및 전문서비스',\n",
    "    '서치펌': '광고·마케팅 및 전문서비스',\n",
    "    '회계': '광고·마케팅 및 전문서비스',\n",
    "    '세무': '광고·마케팅 및 전문서비스',\n",
    "    '컨설팅': '광고·마케팅 및 전문서비스',\n",
    "    '헤드헌팅': '광고·마케팅 및 전문서비스',\n",
    "    '조사': '광고·마케팅 및 전문서비스',\n",
    "    '상담': '광고·마케팅 및 전문서비스',\n",
    "    '아웃소싱': '광고·마케팅 및 전문서비스',\n",
    "    '법무': '광고·마케팅 및 전문서비스',\n",
    "\n",
    "    # 운송·정비·서비스\n",
    "    '운송': '운송·정비·서비스',\n",
    "    'A/S': '운송·정비·서비스',\n",
    "    '항공': '운송·정비·서비스',\n",
    "    '카센터': '운송·정비·서비스',\n",
    "    '정비': '운송·정비·서비스',\n",
    "    '시설관리': '운송·정비·서비스',\n",
    "    '콜센터': '운송·정비·서비스',\n",
    "    '경비': '운송·정비·서비스',\n",
    "\n",
    "    # 인터넷·커뮤니티·포털·디지털 컨텐츠\n",
    "    '커뮤니티': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '컨텐츠': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '포털': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '소셜네트워크서비스(SNS)': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷만화': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '취업포털': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷금융': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷방송': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷생활정보': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷게임': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷영화': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷여행': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷경매': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷부동산': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '여성포털': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷법률': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "\n",
    "    # 숙박·여행·외식·레저\n",
    "    '호텔': '숙박·여행·외식·레저',\n",
    "    '여행': '숙박·여행·외식·레저',\n",
    "    '외식': '숙박·여행·외식·레저',\n",
    "    '음식료': '숙박·여행·외식·레저',\n",
    "    '웨딩': '숙박·여행·외식·레저',\n",
    "    '여가': '숙박·여행·외식·레저',\n",
    "    '레저': '숙박·여행·외식·레저',\n",
    "\n",
    "    # 기타·공공\n",
    "    '기타': '기타·공공',\n",
    "    '공공기관': '기타·공공',\n",
    "    '공기업': '기타·공공',\n",
    "    '상사': '기타·공공',\n",
    "    '상조': '기타·공공',\n",
    "    '원무': '기타·공공',\n",
    "    '단체': '기타·공공',\n",
    "    '협회': '기타·공공',\n",
    "    '중': '기타·공공',\n",
    "    '특수': '기타·공공',\n",
    "}\n",
    "\n",
    "folder_path = './crawling'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 'category' in data:\n",
    "            original_categories = data['category']\n",
    "            corrected_categories = []\n",
    "            # 잘못된 카테고리 수정\n",
    "            for cat in original_categories:\n",
    "                corrected_categories.append(corrections.get(cat, cat))\n",
    "            # 수정된 카테고리로 다시 업데이트\n",
    "            data['category'] = corrected_categories\n",
    "            \n",
    "            # 대분류 매핑 진행\n",
    "            major_categories = set()\n",
    "            for cat in corrected_categories:\n",
    "                major = mapping.get(cat, \"미분류\")\n",
    "                major_categories.add(major)\n",
    "            data['majorCategory'] = list(major_categories)\n",
    "        \n",
    "        # 변경된 내용을 원본 파일에 저장\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"모든 JSON 파일에 카테고리 수정 및 대분류 추가 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매핑에 없는 카테고리:\n"
     ]
    }
   ],
   "source": [
    "# 현재 매핑 사전 (이전 예시 코드에서 사용한 매핑)\n",
    "mapping = {\n",
    "    # IT·디지털 및 소프트웨어\n",
    "    'ERP': 'IT·디지털 및 소프트웨어',\n",
    "    'APP': 'IT·디지털 및 소프트웨어',\n",
    "    '모바일': 'IT·디지털 및 소프트웨어',\n",
    "    '솔루션': 'IT·디지털 및 소프트웨어',\n",
    "    'SI': 'IT·디지털 및 소프트웨어',\n",
    "    '보안': 'IT·디지털 및 소프트웨어',\n",
    "    'IT컨설팅': 'IT·디지털 및 소프트웨어',\n",
    "    'CRM': 'IT·디지털 및 소프트웨어',\n",
    "    '컴퓨터': 'IT·디지털 및 소프트웨어',\n",
    "    '정보보안': 'IT·디지털 및 소프트웨어',\n",
    "    '웹에이전시': 'IT·디지털 및 소프트웨어',\n",
    "    '네트워크': 'IT·디지털 및 소프트웨어',\n",
    "    '통신서비스': 'IT·디지털 및 소프트웨어',\n",
    "\n",
    "    # 제조·기계·산업\n",
    "    '기계': '제조·기계·산업',\n",
    "    '자재': '제조·기계·산업',\n",
    "    '하드웨어': '제조·기계·산업',\n",
    "    '전기': '제조·기계·산업',\n",
    "    '제어': '제조·기계·산업',\n",
    "    '기계설비': '제조·기계·산업',\n",
    "    '금속': '제조·기계·산업',\n",
    "    '전자': '제조·기계·산업',\n",
    "    '디스플레이': '제조·기계·산업',\n",
    "    '장비': '제조·기계·산업',\n",
    "    '인쇄': '제조·기계·산업',\n",
    "    '광학': '제조·기계·산업',\n",
    "    '화학': '제조·기계·산업',\n",
    "    '생활화학': '제조·기계·산업',\n",
    "    '재료': '제조·기계·산업',\n",
    "    '설비': '제조·기계·산업',\n",
    "    '환경': '제조·기계·산업',\n",
    "    '목재': '제조·기계·산업',\n",
    "    '섬유': '제조·기계·산업',\n",
    "    '제지': '제조·기계·산업',\n",
    "    '조선': '제조·기계·산업',\n",
    "    '에너지': '제조·기계·산업',\n",
    "    '자동차': '제조·기계·산업',\n",
    "    'CAD': '제조·기계·산업',\n",
    "    '철강': '제조·기계·산업',\n",
    "    '반도체': '제조·기계·산업',\n",
    "    '식품가공': '제조·기계·산업',\n",
    "\n",
    "    # 건설·부동산·인테리어\n",
    "    '토목': '건설·부동산·인테리어',\n",
    "    '임대': '건설·부동산·인테리어',\n",
    "    '부동산': '건설·부동산·인테리어',\n",
    "    '렌탈': '건설·부동산·인테리어',\n",
    "    '건설': '건설·부동산·인테리어',\n",
    "    '중개': '건설·부동산·인테리어',\n",
    "    '건축': '건설·부동산·인테리어',\n",
    "    '인테리어': '건설·부동산·인테리어',\n",
    "    '조경': '건설·부동산·인테리어',\n",
    "    '시공': '건설·부동산·인테리어',\n",
    "\n",
    "    # 금융·보험\n",
    "    '은행': '금융·보험',\n",
    "    '보험': '금융·보험',\n",
    "    '대출': '금융·보험',\n",
    "    '카드': '금융·보험',\n",
    "    '금융': '금융·보험',\n",
    "    '캐피탈': '금융·보험',\n",
    "    '리스': '금융·보험',\n",
    "    '증권': '금융·보험',\n",
    "\n",
    "    # 유통·소매·물류\n",
    "    '도소매': '유통·소매·물류',\n",
    "    '생활용품': '유통·소매·물류',\n",
    "    '의류': '유통·소매·물류',\n",
    "    '백화점': '유통·소매·물류',\n",
    "    '쇼핑몰': '유통·소매·물류',\n",
    "    '소셜커머스': '유통·소매·물류',\n",
    "    '배송': '유통·소매·물류',\n",
    "    '물류': '유통·소매·물류',\n",
    "    '프랜차이즈': '유통·소매·물류',\n",
    "    '가구': '유통·소매·물류',\n",
    "    '소비재': '유통·소매·물류',\n",
    "    '화장품': '유통·소매·물류',\n",
    "    '패션': '유통·소매·물류',\n",
    "    '무역': '유통·소매·물류',\n",
    "    '오픈마켓': '유통·소매·물류',\n",
    "    '유통': '유통·소매·물류',\n",
    "\n",
    "    # 교육·연구\n",
    "    '교육원': '교육·연구',\n",
    "    '방문교육': '교육·연구',\n",
    "    '초등학교': '교육·연구',\n",
    "    '중학교': '교육·연구',\n",
    "    '고등학교': '교육·연구',\n",
    "    '대학교': '교육·연구',\n",
    "    '특수학교': '교육·연구',\n",
    "    '어학원': '교육·연구',\n",
    "    '학습지': '교육·연구',\n",
    "    '학원': '교육·연구',\n",
    "    '연구': '교육·연구',\n",
    "    '인터넷교육': '교육·연구',\n",
    "\n",
    "    # 의료·보건·복지\n",
    "    '요양': '의료·보건·복지',\n",
    "    '의료(진료과별)': '의료·보건·복지',\n",
    "    '의료(병원분류별)': '의료·보건·복지',\n",
    "    '의료(간호)': '의료·보건·복지',\n",
    "    '보건': '의료·보건·복지',\n",
    "    '제약': '의료·보건·복지',\n",
    "    '바이오': '의료·보건·복지',\n",
    "    '사회복지': '의료·보건·복지',\n",
    "\n",
    "    # 미디어·엔터테인먼트·문화\n",
    "    '영화': '미디어·엔터테인먼트·문화',\n",
    "    '잡지': '미디어·엔터테인먼트·문화',\n",
    "    '출판': '미디어·엔터테인먼트·문화',\n",
    "    '연예': '미디어·엔터테인먼트·문화',\n",
    "    '방송': '미디어·엔터테인먼트·문화',\n",
    "    '음반': '미디어·엔터테인먼트·문화',\n",
    "    '프로덕션': '미디어·엔터테인먼트·문화',\n",
    "    '예술': '미디어·엔터테인먼트·문화',\n",
    "    '전시': '미디어·엔터테인먼트·문화',\n",
    "    '배급': '미디어·엔터테인먼트·문화',\n",
    "    '애니메이션': '미디어·엔터테인먼트·문화',\n",
    "    '신문': '미디어·엔터테인먼트·문화',\n",
    "    '공연': '미디어·엔터테인먼트·문화',\n",
    "    '문화': '미디어·엔터테인먼트·문화',\n",
    "    '스포츠': '미디어·엔터테인먼트·문화',\n",
    "    '사진': '미디어·엔터테인먼트·문화',\n",
    "    '언론사': '미디어·엔터테인먼트·문화',\n",
    "    '케이블': '미디어·엔터테인먼트·문화',\n",
    "    '게임': '미디어·엔터테인먼트·문화',\n",
    "    '엔터테인먼트': '미디어·엔터테인먼트·문화',\n",
    "\n",
    "    # 광고·마케팅 및 전문서비스\n",
    "    '홍보': '광고·마케팅 및 전문서비스',\n",
    "    '광고': '광고·마케팅 및 전문서비스',\n",
    "    '이벤트': '광고·마케팅 및 전문서비스',\n",
    "    '서치펌': '광고·마케팅 및 전문서비스',\n",
    "    '회계': '광고·마케팅 및 전문서비스',\n",
    "    '세무': '광고·마케팅 및 전문서비스',\n",
    "    '컨설팅': '광고·마케팅 및 전문서비스',\n",
    "    '헤드헌팅': '광고·마케팅 및 전문서비스',\n",
    "    '조사': '광고·마케팅 및 전문서비스',\n",
    "    '상담': '광고·마케팅 및 전문서비스',\n",
    "    '아웃소싱': '광고·마케팅 및 전문서비스',\n",
    "    '법무': '광고·마케팅 및 전문서비스',\n",
    "    '디자인': '광고·마케팅 및 전문서비스',\n",
    "\n",
    "    # 운송·정비·서비스\n",
    "    '운송': '운송·정비·서비스',\n",
    "    'A/S': '운송·정비·서비스',\n",
    "    '항공': '운송·정비·서비스',\n",
    "    '카센터': '운송·정비·서비스',\n",
    "    '정비': '운송·정비·서비스',\n",
    "    '시설관리': '운송·정비·서비스',\n",
    "    '콜센터': '운송·정비·서비스',\n",
    "    '경비': '운송·정비·서비스',\n",
    "\n",
    "    # 인터넷·커뮤니티·포털·디지털 컨텐츠\n",
    "    '커뮤니티': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '컨텐츠': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '포털': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '소셜네트워크서비스(SNS)': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷만화': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '취업포털': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷금융': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷방송': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷생활정보': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷게임': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷영화': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷여행': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷경매': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷부동산': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '여성포털': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "    '인터넷법률': '인터넷·커뮤니티·포털·디지털 컨텐츠',\n",
    "\n",
    "    # 숙박·여행·외식·레저\n",
    "    '호텔': '숙박·여행·외식·레저',\n",
    "    '여행': '숙박·여행·외식·레저',\n",
    "    '외식': '숙박·여행·외식·레저',\n",
    "    '음식료': '숙박·여행·외식·레저',\n",
    "    '웨딩': '숙박·여행·외식·레저',\n",
    "    '여가': '숙박·여행·외식·레저',\n",
    "    '레저': '숙박·여행·외식·레저',\n",
    "\n",
    "    # 기타·공공\n",
    "    '기타': '기타·공공',\n",
    "    '공공기관': '기타·공공',\n",
    "    '공기업': '기타·공공',\n",
    "    '상사': '기타·공공',\n",
    "    '상조': '기타·공공',\n",
    "    '원무': '기타·공공',\n",
    "    '단체': '기타·공공',\n",
    "    '협회': '기타·공공',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# corrected_categories 내에서 mapping에 없는 항목 찾기\n",
    "missing = [cat for cat in corrected_categories if cat not in mapping]\n",
    "\n",
    "print(\"매핑에 없는 카테고리:\")\n",
    "for cat in missing:\n",
    "    print(\"-\", cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder_path = './crawling'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 'category' in data:\n",
    "            original_categories = data['category']\n",
    "            corrected_categories = []\n",
    "            # 잘못된 카테고리 수정 (예시: corrections 사전 적용)\n",
    "            for cat in original_categories:\n",
    "                corrected_categories.append(corrections.get(cat, cat))\n",
    "            data['category'] = corrected_categories\n",
    "            \n",
    "            # 대분류 매핑 진행 및 기존 majorCategory 덮어쓰기\n",
    "            major_categories = set()\n",
    "            for cat in corrected_categories:\n",
    "                major = mapping.get(cat, \"미분류\")\n",
    "                major_categories.add(major)\n",
    "            data['majorCategory'] = list(major_categories)\n",
    "        \n",
    "        # 변경된 내용을 파일에 저장 (기존 majorCategory 필드는 덮어씌워짐)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"모든 JSON 파일의 대분류(majorCategory)를 업데이트 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카테고리에 '미분류'가 포함된 기업들:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder_path = './crawling'\n",
    "companies_with_go = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        # \"category\" 리스트에 '고'가 포함되어 있으면 해당 기업명을 리스트에 추가\n",
    "        if 'category' in data and any(cat == '미분류' for cat in data['category']):\n",
    "            companies_with_go.append(data.get(\"companyName\", file_name))\n",
    "\n",
    "print(\"카테고리에 '미분류'가 포함된 기업들:\")\n",
    "for company in companies_with_go:\n",
    "    print(\"-\", company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 JSON 파일에서 학교 관련 카테고리 통일 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 학교 관련 카테고리 통일 매핑 정의\n",
    "school_corrections = {\n",
    "    \"학교(초)\": \"초등학교\",\n",
    "    \"중\": \"중학교\",\n",
    "    \"고\": \"고등학교\",\n",
    "    \"대학\": \"대학교\",\n",
    "    \"특수\": \"특수학교\"\n",
    "    # 만약 \"특수\"도 학교를 의미한다면, 추가로 변환할 수 있음\n",
    "    # 예: \"특수\": \"특수학교\"\n",
    "}\n",
    "\n",
    "folder_path = './crawling'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 'category' in data:\n",
    "            updated_categories = []\n",
    "            for cat in data['category']:\n",
    "                # school_corrections에 있으면 변환, 없으면 그대로 사용\n",
    "                updated_categories.append(school_corrections.get(cat, cat))\n",
    "            data['category'] = updated_categories\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"모든 JSON 파일에서 학교 관련 카테고리 통일 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majorCategory 목록을 'majorCategory_list.json' 파일로 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder_path = './crawling'\n",
    "all_major_categories = set()\n",
    "\n",
    "# 폴더 내 모든 JSON 파일 순회\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        # majorCategory 필드가 있으면 항목들을 집합에 추가\n",
    "        if 'majorCategory' in data:\n",
    "            for major in data['majorCategory']:\n",
    "                all_major_categories.add(major)\n",
    "\n",
    "# 유니크한 majorCategory 목록을 리스트로 변환\n",
    "major_category_list = list(all_major_categories)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "output_file = 'majorCategory_list.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(major_category_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"majorCategory 목록을 '{output_file}' 파일로 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
